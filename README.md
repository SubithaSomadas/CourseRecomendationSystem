# CourseRecomendationSystem
OBJECTIVE

The primary goal is to develop a user-friendly course recommendation system that utilizes cosine similarity to analyze course descriptions, skills, and difficulty levels. This system aims to offer personalized course suggestions based on user preferences, ultimately enhancing the learning experience and promoting continuous education.

This project is a combined from the existing github course recomendation project with web page that encapsulates the innovative development of a sophisticated course recommendation system leveraging the power of cosine similarity within the realm of machine learning. Centered around a comprehensive Coursera dataset procured from Kaggle, this endeavor aims to revolutionize the way users explore and engage with educational content. Through intricate processes of text vectorization and nuanced cosine similarity computations, this system ingeniously deciphers course descriptions, skill sets, and difficulty levels to offer users tailored recommendations. However, the true novelty lies in the seamless integration of this backend complexity into a user-centric interface, brought to life using Streamlit. This interface acts as the gateway for users to effortlessly input their preferences and interests, subsequently receiving personalized course suggestions that align closely with their learning objectives. By bridging the gap between intricate machine learning mechanisms and user accessibility, this project stands as a testament to the fusion of technical prowess with practical usability, aiming to redefine the educational landscape and empower learners in their quest for knowledge and skill enhancement.

IMPLEMENTATION
Dataset 

The project employs the Coursera dataset that consists of 3522 entries across seven columns. These columns encompass attributes such as 'Course Name,' 'University,' 'Difficulty Level,' 'Course Rating,' 'Course URL,' 'Course Description,' and 'Skills,' all of which are of 'object' data type. Data pre-processing involves cleaning text by removing special characters, spaces, and standardizing the format for effective vectorization.

 
Course Name		: The Name of the Course.
University 		: The University or Industry Partner that offers the Course.
Difficulty Level	: The Difficulty Level of the Course. Has 3 values: Beginner, Intermediate, Advanced. Also has missing values represented by Not Calibrated.
Course Rating	: Rating on a 5-point scale with minimum step value 0.1. Missing values represented by Not Calibrated.
Course URL		: The Course URL.
Course Description	: The Description of the Course. Has Missing Values.
Skills	: The Skill Tags associated with the Course extracted through NLP processes.


run commands:
> python recomend_page.py

> streamlit run recomend_page.py

Data Pre-processing

The data preprocessing steps undertaken involved an initial inspection and understanding of the dataset structure. With a total of 3522 entries across seven distinct columns, the dataset exhibited a comprehensive scope, encapsulating crucial attributes pivotal for course recommendation. These attributes included 'Course Name,' 'University,' 'Difficulty Level,' 'Course Rating,' 'Course URL,' 'Course Description,' and 'Skills.' Notably, all columns contained 'object' data types, signifying diverse textual or categorical information. Subsequent examinations revealed the absence of any missing or null values within the dataset, affirming its completeness and reliability for further analysis. Analysis of categorical attributes like 'Difficulty Level' unveiled the distribution of courses across proficiency levels, ranging from 'Beginner,' 'Advanced,' 'Intermediate,' 'Conversant,' to 'Not Calibrated,' offering insights into the variety of skill levels catered to within the dataset. Moreover, a comprehensive view of course ratings and their frequencies demonstrated the varied feedback courses received, while an assessment of the universities' course offerings shed light on the diverse educational institutions contributing to the dataset, with 'Coursera Project Network' notably leading in course provision. These meticulous steps in data preprocessing established a robust foundation for subsequent analytical and recommendation system development, ensuring a comprehensive understanding of the dataset's attributes and distributions.
 
Text Vectorization 

Text vectorization is a pivotal process in natural language processing, crucial for transforming textual data into a numerical format understandable by machine learning algorithms. In the provided snippet, the CountVectorizer from the Scikit-learn library is employed to convert a collection of text, specifically the 'tags' column from the DataFrame 'new_df,' into a matrix of token counts. This transformation involves several key steps. First, by specifying 'max_features=5000' and 'stop_words='english',' the CountVectorizer constrains the vocabulary size to the top 5000 frequently occurring words while excluding common English stop words.
The resulting 'vectors' represent a sparse matrix, where each row corresponds to a document or text entry, and each column signifies a unique word or token from the text corpus. The matrix entries indicate the frequency of each word's occurrence within the respective document. This representation efficiently captures the essence of the text data in a structured numerical form, facilitating subsequent machine learning operations.
Converting the sparse matrix 'vectors' to a dense matrix using 'vectors.todense()' creates a more human-readable format, allowing exploration and analysis of the transformed data. The resultant dense matrix, 'df_cv_words,' comprising rows and columns equivalent to the number of documents and unique words respectively, forms the foundation for further analysis. Each cell in this matrix contains a count representing the frequency of occurrence of a specific word within a particular document. Ultimately, 'df_cv_words' encapsulates the textual data in a format amenable for modeling, enabling downstream machine learning algorithms to operate effectively on the transformed text data.

Similarity Calculation

Cosine similarity stands as a fundamental metric used to quantify the similarity between two non-zero vectors, frequently employed in natural language processing and recommendation systems. the cosine_similarity function from Scikit-learn computes the cosine similarity matrix based on the previously transformed text data, 'vectors.' This matrix represents pairwise similarities between all documents or courses in the dataset.
The cosine similarity metric measures the cosine of the angle between two vectors in a multi-dimensional space, specifically here, the angle between the vectors representing different courses in a high-dimensional space of words or tags. This calculation assesses the orientation and similarity of these vectors irrespective of their magnitude, focusing solely on their directional agreement. A value of 1 indicates perfect similarity (vectors align), 0 signifies orthogonality (vectors are perpendicular), and -1 denotes complete dissimilarity (vectors are diametrically opposed).
Further, the generation of 'course_indices' utilizing the 'pd.Series' function and 'index' assignment enables the mapping of course names to their corresponding indices within the dataset, facilitating efficient retrieval of specific course information based on their names. For instance, the extraction of the index for 'Finance for Managers' from the 'course_indices' series obtains the index position within the dataset corresponding to this particular course, aiding in subsequent operations or analyses requiring specific course identification.
Overall, cosine similarity provides a robust measure to assess the likeness or resemblance between different courses, forming the foundation for recommendation systems by identifying courses with similar content or characteristics based on their textual representation.
Cosine Similarity
Cosine similarity is a measure used to determine the similarity between two non-zero vectors in a multi-dimensional space. Specifically prevalent in text analysis and recommendation systems, it assesses the directional similarity rather than the magnitude or length of vectors.
Imagine each vector representing a document, where each dimension corresponds to a word or a feature present in the dataset. Cosine similarity calculates the cosine of the angle between these vectors, determining how similar or dissimilar they are based on the orientation of their directions in this high-dimensional space.
The cosine similarity value ranges between -1 and 1. A cosine similarity of 1 indicates that the vectors are in the same direction, implying perfect similarity. A value of -1 signifies complete dissimilarity, where the vectors are in opposite directions. A score of 0 suggests orthogonality, meaning the vectors are perpendicular to each other and have no apparent similarity.
Mathematically, cosine similarity is computed as the dot product of the two vectors divided by the product of their magnitudes. This calculation normalizes the similarity measure, focusing solely on the angle between the vectors, making it effective for comparing documents or items based on their content or features.
In the context of text data, cosine similarity assesses the similarity of documents by comparing their textual content, where each word or term represents a dimension in the vector space. It's widely used in tasks like information retrieval, recommendation systems, and natural language processing to determine the resemblance or relatedness between texts, aiding in tasks such as document similarity, clustering, and content-based recommendations.

Exporting the Model

Utilizing Python's 'pickle' module, the model separate files, ensuring their storage and accessibility beyond the current session. The 'pickle.dump()' function serializes the data, allowing it to be written into files in binary format ('wb' mode), preserving the structure and content.
The first 'pickle.dump()' operation stores the 'cosine_similarity_mat,' representing the computed cosine similarity matrix of courses, into a file named 'similarity.pkl.' This matrix encapsulates the core essence of the recommendation engine, containing the pairwise similarities between all courses within the dataset. Saving this matrix ensures its availability for subsequent recommendation computations or analyses without the need to recompute similarities, thereby optimizing computational resources.
The second 'pickle.dump()' operation saves 'new_df'—the DataFrame representing course data, including 'Course Name,' 'Tags,' and other relevant attributes—transformed into a dictionary format, into a file named 'course_list.pkl.' This conversion into a dictionary form enables easy access to course information in a structured manner, facilitating potential usage scenarios where dictionary-based data retrieval is more convenient.
Lastly, the third 'pickle.dump()' operation directly saves the 'new_df' DataFrame itself into a file named 'courses.pkl.' This action preserves the entire DataFrame object, allowing seamless retrieval and utilization of the entire dataset, preserving its structure and integrity for any future analyses, model retraining, or system enhancements.

Web Interface Development

The implementation involves creating a web interface using Streamlit, designed to facilitate course recommendations based on user input. The interface, created by integrating various components and functionalities, starts by loading essential data: 'courses_list' and 'similarity.' 'courses_list' likely contains information about available courses, while 'similarity' might store a precomputed matrix or data structure capturing the similarity scores between these courses, likely derived using techniques like cosine similarity.
The central function 'recommend' is responsible for fetching recommendations. It takes a selected course as input, retrieves its index within the dataset, and computes similarity scores between the selected course and others in the dataset. This function then sorts these similarities to suggest courses with the highest similarity to the user's selection.
Within the 'main' function, the Streamlit elements are orchestrated. The interface features a title and attribution tags, styled using HTML markup. Users can select a course of interest via a dropdown menu and specify the number of recommended courses to display using a numeric input slider. Upon clicking the 'Recommended Courses' button, the system triggers the 'recommend' function, generating a sublist of recommended courses based on user preferences.
The 'st.subheader' and 'st.text' functions then display the recommended course names in the interface. The code systematically iterates through the recommended courses and exhibits them within the Streamlit interface.
Overall, this web interface leverages Streamlit's simplicity to create an interactive platform allowing users to select courses and receive personalized recommendations, thereby streamlining the course discovery process. The backend functionalities, combined with the user-friendly interface, create an accessible system for users to explore and discover courses aligned with their interests.

Conclusion

This project stands as a testament to the fusion of advanced machine learning techniques and user-centric interface design, culminating in a sophisticated yet accessible course recommendation system. By harnessing the power of cosine similarity and adeptly manipulating textual data, this system revolutionizes the way users engage with educational content. The robust backend, encapsulated in the Python code, processes a diverse dataset of over 3500 courses, extracting nuances from 'Course Name,' 'University,' 'Difficulty Level,' 'Course Rating,' 'Course URL,' 'Course Description,' and 'Skills' columns to create a comprehensive matrix of course similarities.
However, the true essence of this project transcends the intricacies of data manipulation. It lies in the user-facing interface crafted using Streamlit, where simplicity meets functionality. This interface, with its polished design and intuitive controls, empowers users to navigate through a plethora of courses effortlessly. The dropdown menu allowing course selection and the numeric input slider to specify recommendation count pave the way for a seamless user experience.
Moreover, the 'Recommend Courses' button triggers a backend recommendation engine that swiftly computes and presents tailored suggestions, transforming the overwhelming array of courses into a curated selection finely attuned to each user's preferences. As the system sifts through the dataset, the backend's cosine similarity computations pinpoint courses akin to the user's selection, unveiling a bouquet of recommendations perfectly aligned with their interests.
In essence, this project isn't merely about course suggestions. It's a convergence of technical prowess and user convenience, paving a pathway for personalized learning experiences. By simplifying the labyrinth of educational choices, it fosters a culture of continuous learning and skill enhancement. It caters to the novice seeking an introductory course or the expert delving into specialized domains, all unified by a single interface built for exploration and knowledge acquisition.
Ultimately, this project bridges the gap between intricate machine learning algorithms and practical usability, epitomizing the synergy between technology and human interaction. It doesn't just recommend courses; it opens the door to a world of learning possibilities, poised to revolutionize how individuals discover and engage with educational content, fostering a future where learning knows no bounds.

